"""
Advanced Music Mixer
í”„ë¡œí˜ì…”ë„ DJ ìŠ¤íƒ€ì¼ì˜ ê³ ê¸‰ ìŒì•… ë¯¹ì‹± ì—”ì§„
"""

import numpy as np
import librosa
import soundfile as sf
from pydub import AudioSegment
import pyrubberband as pyrb
from pedalboard import Pedalboard, HighpassFilter, LowpassFilter, Gain
from typing import Dict, Tuple, Optional
from audio_analyzer import AudioAnalyzer


class AdvancedMixer:
    """ì „ë¬¸ê°€ê¸‰ ìŒì•… ë¯¹ì‹± ì—”ì§„"""
    
    def __init__(self):
        self.sample_rate = 44100
    
    def load_audio(self, file_path: str) -> Tuple[np.ndarray, int]:
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ
        
        Args:
            file_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            (audio_data, sample_rate)
        """
        y, sr = librosa.load(file_path, sr=self.sample_rate, mono=False)
        
        # ìŠ¤í…Œë ˆì˜¤ë¡œ ë³€í™˜ (ëª¨ë…¸ì¸ ê²½ìš°)
        if y.ndim == 1:
            y = np.stack([y, y])
        
        return y, sr
    
    def match_tempo(self, audio: np.ndarray, original_bpm: float, target_bpm: float) -> np.ndarray:
        """
        í…œí¬ ë§¤ì¹­ (time-stretching without pitch change)
        
        Args:
            audio: ì˜¤ë””ì˜¤ ë°ì´í„°
            original_bpm: ì›ë³¸ BPM
            target_bpm: ëª©í‘œ BPM
            
        Returns:
            í…œí¬ê°€ ì¡°ì •ëœ ì˜¤ë””ì˜¤
        """
        if abs(original_bpm - target_bpm) < 0.5:
            print(f"  â­ï¸  Tempo already matched ({original_bpm:.1f} BPM)")
            return audio
        
        rate = target_bpm / original_bpm
        print(f"  ğŸšï¸  Stretching tempo: {original_bpm:.1f} â†’ {target_bpm:.1f} BPM (rate: {rate:.3f})")
        
        # pyrubberbandë¡œ ê³ í’ˆì§ˆ time-stretching
        # ìŠ¤í…Œë ˆì˜¤ ì²˜ë¦¬
        if audio.ndim == 2:
            stretched = np.zeros_like(audio)
            for ch in range(audio.shape[0]):
                stretched[ch] = pyrb.time_stretch(audio[ch], self.sample_rate, rate)
        else:
            stretched = pyrb.time_stretch(audio, self.sample_rate, rate)
        
        return stretched
    
    def align_beats(self, audio1: np.ndarray, audio2: np.ndarray, 
                    beats1: np.ndarray, beats2: np.ndarray,
                    transition_point: float) -> Tuple[np.ndarray, np.ndarray, float]:
        """
        ë¹„íŠ¸ ì •ë ¬ - ë‘ íŠ¸ë™ì˜ ë¹„íŠ¸ë¥¼ ì™„ë²½í•˜ê²Œ ë™ê¸°í™”
        
        Args:
            audio1: ì²« ë²ˆì§¸ ì˜¤ë””ì˜¤
            audio2: ë‘ ë²ˆì§¸ ì˜¤ë””ì˜¤
            beats1: ì²« ë²ˆì§¸ ì˜¤ë””ì˜¤ì˜ ë¹„íŠ¸ íƒ€ì„ìŠ¤íƒ¬í”„
            beats2: ë‘ ë²ˆì§¸ ì˜¤ë””ì˜¤ì˜ ë¹„íŠ¸ íƒ€ì„ìŠ¤íƒ¬í”„
            transition_point: ì „í™˜ ì‹œì‘ ì§€ì  (ì´ˆ)
            
        Returns:
            (ì •ë ¬ëœ audio1, ì •ë ¬ëœ audio2, ì¡°ì •ëœ ì „í™˜ ì§€ì )
        """
        print(f"  ğŸ¯ Aligning beats at transition point: {transition_point:.2f}s")
        
        # ì „í™˜ ì§€ì ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ë¹„íŠ¸ ì°¾ê¸°
        if len(beats1) > 0:
            closest_beat1_idx = np.argmin(np.abs(beats1 - transition_point))
            aligned_point1 = beats1[closest_beat1_idx]
        else:
            aligned_point1 = transition_point
        
        # audio2ëŠ” ì‹œì‘ ë¹„íŠ¸ì— ë§ì¶¤
        if len(beats2) > 0:
            first_beat2 = beats2[0]
        else:
            first_beat2 = 0
        
        print(f"  âœ“ Beat alignment: Track1 @ {aligned_point1:.2f}s, Track2 @ {first_beat2:.2f}s")
        
        return audio1, audio2, aligned_point1
    
    def find_optimal_transition_point(self, analysis1: Dict, analysis2: Dict, 
                                     transition_bars: int = 16) -> Tuple[float, float]:
        """
        ìµœì ì˜ ì „í™˜ ì§€ì  ì°¾ê¸°
        
        Args:
            analysis1: ì²« ë²ˆì§¸ íŠ¸ë™ ë¶„ì„ ê²°ê³¼
            analysis2: ë‘ ë²ˆì§¸ íŠ¸ë™ ë¶„ì„ ê²°ê³¼
            transition_bars: ì „í™˜ ê¸¸ì´ (ë°” ë‹¨ìœ„)
            
        Returns:
            (track1_out_point, track2_in_point)
        """
        print(f"  ğŸ” Finding optimal transition point ({transition_bars} bars)...")
        
        # Track 1: ì•„ì›ƒíŠ¸ë¡œ ì‹œì‘ì  ì‚¬ìš©
        outro1 = analysis1['segments']['outro']
        mixout_point = outro1['start']
        
        # Track 2: ì¸íŠ¸ë¡œ ë ì§€ì  ì‚¬ìš©
        intro2 = analysis2['segments']['intro']
        mixin_point = intro2['end'] if intro2['end'] > 5 else 0
        
        # ë¹„íŠ¸ì— ë§ì¶° ì¡°ì •
        if len(analysis1['beats']) > 0:
            closest_beat_idx = np.argmin(np.abs(analysis1['beats'] - mixout_point))
            mixout_point = analysis1['beats'][closest_beat_idx]
        
        print(f"  âœ“ Transition points: Track1 @ {mixout_point:.2f}s, Track2 @ {mixin_point:.2f}s")
        
        return mixout_point, mixin_point
    
    def create_crossfade(self, audio1: np.ndarray, audio2: np.ndarray,
                        mixout_point: float, mixin_point: float,
                        crossfade_duration: float,
                        style: str = 'classic') -> np.ndarray:
        """
        í¬ë¡œìŠ¤í˜ì´ë“œ ìƒì„±
        
        Args:
            audio1: ì²« ë²ˆì§¸ ì˜¤ë””ì˜¤
            audio2: ë‘ ë²ˆì§¸ ì˜¤ë””ì˜¤
            mixout_point: Track1 ë¯¹ìŠ¤ ì•„ì›ƒ ì‹œì‘ì 
            mixin_point: Track2 ë¯¹ìŠ¤ ì¸ ì‹œì‘ì 
            crossfade_duration: í¬ë¡œìŠ¤í˜ì´ë“œ ê¸¸ì´ (ì´ˆ)
            style: ì „í™˜ ìŠ¤íƒ€ì¼ ('classic', 'bass_swap', 'filter_sweep')
            
        Returns:
            ë¯¹ìŠ¤ëœ ì˜¤ë””ì˜¤
        """
        print(f"  ğŸ›ï¸  Creating {style} crossfade ({crossfade_duration:.1f}s)...")
        
        # ìƒ˜í”Œ ë‹¨ìœ„ë¡œ ë³€í™˜
        mixout_sample = int(mixout_point * self.sample_rate)
        mixin_sample = int(mixin_point * self.sample_rate)
        fade_samples = int(crossfade_duration * self.sample_rate)
        
        # Track2 ì‹œì‘ ìœ„ì¹˜ ê³„ì‚°
        track2_start_in_mix = mixout_sample
        
        # ìµœì¢… ê¸¸ì´ ê³„ì‚°
        total_length = max(
            audio1.shape[1],
            track2_start_in_mix + (audio2.shape[1] - mixin_sample)
        )
        
        # ì¶œë ¥ ë²„í¼ ìƒì„± (ìŠ¤í…Œë ˆì˜¤)
        mixed = np.zeros((2, total_length), dtype=np.float32)
        
        # Track1 ë³µì‚¬ (ì „ì²´)
        mixed[:, :audio1.shape[1]] = audio1
        
        # í¬ë¡œìŠ¤í˜ì´ë“œ êµ¬ê°„ ê³„ì‚°
        fade_start = mixout_sample
        fade_end = min(fade_start + fade_samples, audio1.shape[1])
        actual_fade_samples = fade_end - fade_start
        
        if style == 'classic':
            # Classic crossfade
            fade_out_curve = np.linspace(1, 0, actual_fade_samples)
            fade_in_curve = np.linspace(0, 1, actual_fade_samples)
            
            # Track1 í˜ì´ë“œ ì•„ì›ƒ
            for ch in range(2):
                mixed[ch, fade_start:fade_end] *= fade_out_curve
            
            # Track2 ì¶”ê°€ (í˜ì´ë“œ ì¸)
            track2_fade_start = mixin_sample
            track2_fade_end = min(track2_fade_start + actual_fade_samples, audio2.shape[1])
            track2_fade_samples = track2_fade_end - track2_fade_start
            
            for ch in range(2):
                fade_in_actual = np.linspace(0, 1, track2_fade_samples)
                mixed[ch, fade_start:fade_start + track2_fade_samples] += \
                    audio2[ch, track2_fade_start:track2_fade_end] * fade_in_actual
            
            # Track2 ë‚˜ë¨¸ì§€ ë¶€ë¶„
            remaining_start = fade_start + track2_fade_samples
            remaining_audio2_start = track2_fade_end
            remaining_length = min(
                audio2.shape[1] - remaining_audio2_start,
                total_length - remaining_start
            )
            
            if remaining_length > 0:
                mixed[:, remaining_start:remaining_start + remaining_length] = \
                    audio2[:, remaining_audio2_start:remaining_audio2_start + remaining_length]
        
        elif style == 'bass_swap':
            # Bass swap: ì €ìŒ ë¨¼ì € êµì²´
            print("    ğŸ”Š Applying bass swap...")
            
            # ì €ìŒ/ê³ ìŒ ë¶„ë¦¬ (ê°„ë‹¨í•œ í•„í„°)
            # Track1 ì €ìŒ í˜ì´ë“œ ì•„ì›ƒ
            fade_out_curve = np.linspace(1, 0, actual_fade_samples)
            for ch in range(2):
                mixed[ch, fade_start:fade_end] *= fade_out_curve
            
            # Track2 ì €ìŒ ë¨¼ì € í˜ì´ë“œ ì¸ (ë¹ ë¥´ê²Œ)
            bass_fade_samples = actual_fade_samples // 2
            track2_fade_start = mixin_sample
            
            for ch in range(2):
                bass_fade_in = np.linspace(0, 1, bass_fade_samples)
                mixed[ch, fade_start:fade_start + bass_fade_samples] += \
                    audio2[ch, track2_fade_start:track2_fade_start + bass_fade_samples] * bass_fade_in
            
            # ë‚˜ë¨¸ì§€ ì£¼íŒŒìˆ˜ í˜ì´ë“œ ì¸
            remaining_fade_samples = actual_fade_samples - bass_fade_samples
            for ch in range(2):
                remaining_fade_in = np.linspace(0, 1, remaining_fade_samples)
                mixed[ch, fade_start + bass_fade_samples:fade_end] += \
                    audio2[ch, track2_fade_start + bass_fade_samples:track2_fade_start + actual_fade_samples] * remaining_fade_in
            
            # Track2 ë‚˜ë¨¸ì§€
            remaining_start = fade_end
            remaining_audio2_start = track2_fade_start + actual_fade_samples
            remaining_length = min(
                audio2.shape[1] - remaining_audio2_start,
                total_length - remaining_start
            )
            
            if remaining_length > 0:
                mixed[:, remaining_start:remaining_start + remaining_length] = \
                    audio2[:, remaining_audio2_start:remaining_audio2_start + remaining_length]
        
        else:  # filter_sweep or other
            # ê¸°ë³¸ classic ì‚¬ìš©
            fade_out_curve = np.linspace(1, 0, actual_fade_samples)
            fade_in_curve = np.linspace(0, 1, actual_fade_samples)
            
            for ch in range(2):
                mixed[ch, fade_start:fade_end] *= fade_out_curve
            
            track2_fade_start = mixin_sample
            track2_fade_end = min(track2_fade_start + actual_fade_samples, audio2.shape[1])
            track2_fade_samples = track2_fade_end - track2_fade_start
            
            for ch in range(2):
                fade_in_actual = np.linspace(0, 1, track2_fade_samples)
                mixed[ch, fade_start:fade_start + track2_fade_samples] += \
                    audio2[ch, track2_fade_start:track2_fade_end] * fade_in_actual
            
            remaining_start = fade_start + track2_fade_samples
            remaining_audio2_start = track2_fade_end
            remaining_length = min(
                audio2.shape[1] - remaining_audio2_start,
                total_length - remaining_start
            )
            
            if remaining_length > 0:
                mixed[:, remaining_start:remaining_start + remaining_length] = \
                    audio2[:, remaining_audio2_start:remaining_audio2_start + remaining_length]
        
        print(f"  âœ“ Crossfade complete: {total_length / self.sample_rate:.2f}s total")
        
        return mixed
    
    def normalize_audio(self, audio: np.ndarray, target_db: float = -14.0) -> np.ndarray:
        """
        ì˜¤ë””ì˜¤ ì •ê·œí™”
        
        Args:
            audio: ì˜¤ë””ì˜¤ ë°ì´í„°
            target_db: ëª©í‘œ dB ë ˆë²¨
            
        Returns:
            ì •ê·œí™”ëœ ì˜¤ë””ì˜¤
        """
        # RMS ê³„ì‚°
        rms = np.sqrt(np.mean(audio ** 2))
        
        if rms > 0:
            current_db = 20 * np.log10(rms)
            gain_db = target_db - current_db
            gain_linear = 10 ** (gain_db / 20)
            
            normalized = audio * gain_linear
            
            # í´ë¦¬í•‘ ë°©ì§€
            max_val = np.max(np.abs(normalized))
            if max_val > 1.0:
                normalized = normalized / max_val * 0.99
            
            print(f"  ğŸ”Š Normalized: {current_db:.1f} dB â†’ {target_db:.1f} dB")
            return normalized
        
        return audio
    
    def mix(self, track1_path: str, track2_path: str, output_path: str,
            sync_beats: bool = True,
            match_tempo: bool = True,
            harmonic_mix: bool = True,
            transition_bars: int = 16,
            transition_style: str = 'classic',
            auto_detect: bool = True) -> str:
        """
        ë‘ íŠ¸ë™ì„ ë¯¹ì‹±
        """
        return self.mix_playlist([track1_path, track2_path], output_path, 
                                sync_beats, match_tempo, harmonic_mix, 
                                transition_bars, transition_style, auto_detect)

    def mix_playlist(self, track_paths: list, output_path: str,
                    sync_beats: bool = True,
                    match_tempo: bool = True,
                    harmonic_mix: bool = True,
                    transition_bars: int = 16,
                    transition_style: str = 'classic',
                    auto_detect: bool = True) -> str:
        """
        ì—¬ëŸ¬ íŠ¸ë™ì„ ìˆœì°¨ì ìœ¼ë¡œ ë¯¹ì‹± (í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ë°©ì‹)
        
        Args:
            track_paths: íŠ¸ë™ íŒŒì¼ ê²½ë¡œ ëª©ë¡
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
            ... (ê¸°íƒ€ ì˜µì…˜ì€ mixì™€ ë™ì¼)
        """
        if not track_paths:
            raise ValueError("No tracks provided for mixing")
            
        if len(track_paths) == 1:
            # íŠ¸ë™ì´ í•˜ë‚˜ë©´ ê·¸ëƒ¥ ë³µì‚¬(í¬ë§· ë³€í™˜)ë§Œ í•¨
            audio, sr = self.load_audio(track_paths[0])
            sf.write(output_path, audio.T, self.sample_rate)
            return output_path

        print(f"\n{'='*70}")
        print(f"ğŸ§ Playlist Mixing ({len(track_paths)} tracks)")
        print(f"{'='*70}\n")

        # ì²« ë²ˆì§¸ íŠ¸ë™ ë¡œë“œ ë° ì´ˆê¸°í™”
        current_audio, sr = self.load_audio(track_paths[0])
        analyzer = AudioAnalyzer(track_paths[0])
        current_analysis = analyzer.analyze_full()
        
        # ì²« ë²ˆì§¸ íŠ¸ë™ ì •ê·œí™”
        current_audio = self.normalize_audio(current_audio)
        
        # í…œí¬ ê¸°ì¤€ (ì²« íŠ¸ë™ ë˜ëŠ” í‰ê· ìœ¼ë¡œ ì„¤ì • ê°€ëŠ¥, ì—¬ê¸°ì„  ì²« íŠ¸ë™ ê¸°ì¤€)
        reference_bpm = current_analysis['bpm']

        for i in range(1, len(track_paths)):
            next_track_path = track_paths[i]
            print(f"\nğŸ“ Mixing in Track {i+1}: {os.path.basename(next_track_path)}")
            
            # ë‹¤ìŒ íŠ¸ë™ ë¶„ì„ ë° ë¡œë“œ
            next_analyzer = AudioAnalyzer(next_track_path)
            next_analysis = next_analyzer.analyze_full()
            next_audio, sr_next = self.load_audio(next_track_path)
            
            # í…œí¬ ë§¤ì¹­ (ì´ì „ íŠ¸ë™ì˜ BPMì— ë§ì¶¤)
            if match_tempo:
                next_audio = self.match_tempo(next_audio, next_analysis['bpm'], reference_bpm)
                # ë¹„íŠ¸ ì •ë³´ ì—…ë°ì´íŠ¸
                tempo_ratio = reference_bpm / next_analysis['bpm']
                next_analysis['beats'] = next_analysis['beats'] / tempo_ratio

            # í•˜ëª¨ë‹‰ ì²´í¬ (ì°¸ê³ ìš©)
            if harmonic_mix:
                comp, reason = AudioAnalyzer.are_keys_compatible(current_analysis['camelot'], next_analysis['camelot'])
                print(f"  ğŸ¹ Key: {next_analysis['full_key']} ({next_analysis['camelot']}) -> " + ("âœ… Compatible" if comp else f"âš ï¸ {reason}"))

            # ì „í™˜ì  ê³„ì‚°
            if auto_detect:
                mixout_point, mixin_point = self.find_optimal_transition_point(current_analysis, next_analysis, transition_bars)
            else:
                bars_duration = (60 / reference_bpm) * 4 * transition_bars
                mixout_point = (current_audio.shape[1] / self.sample_rate) - bars_duration
                mixin_point = 0

            # ë¹„íŠ¸ ì •ë ¬
            if sync_beats:
                current_audio, next_audio, mixout_point = self.align_beats(
                    current_audio, next_audio,
                    current_analysis['beats'], next_analysis['beats'],
                    mixout_point
                )

            # í¬ë¡œìŠ¤í˜ì´ë“œ ìƒì„±
            crossfade_duration = (60 / reference_bpm) * 4 * transition_bars
            
            # mixout_point ì´í›„ì˜ beatsëŠ” audio2ì˜ beatsë¡œ ëŒ€ì²´ë˜ê±°ë‚˜ offset ë˜ì–´ì•¼ í•¨ (ì—¬ê¸°ì„  ë‹¨ìˆœ ëˆ„ì )
            current_audio = self.create_crossfade(
                current_audio, next_audio,
                mixout_point, mixin_point,
                crossfade_duration,
                transition_style
            )
            
            # ë‹¤ìŒ ë¯¹ì‹±ì„ ìœ„í•œ current_analysis ì—…ë°ì´íŠ¸
            # ë¯¹ìŠ¤ëœ ê²°ê³¼ë¬¼ì˜ ìƒˆë¡œìš´ ë¹„íŠ¸ì™€ ë¶„ì„ ë°ì´í„°ê°€ í•„ìš”í•˜ì§€ë§Œ, 
            # ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ Track2ì˜ ë°ì´í„°ë¥¼ offset ì‹œì¼œì„œ ì‚¬ìš©
            offset = mixout_point - mixin_point
            current_analysis = {
                'bpm': reference_bpm,
                'beats': next_analysis['beats'] + offset,
                'segments': {
                    'outro': {
                        'start': next_analysis['segments']['outro']['start'] + offset,
                        'end': next_analysis['segments']['outro']['end'] + offset
                    }
                },
                'camelot': next_analysis['camelot']
            }
            
            print(f"  âœ“ Track {i+1} merged. Current total length: {current_audio.shape[1]/self.sample_rate:.1f}s")

        # ìµœì¢… ì •ê·œí™”
        current_audio = self.normalize_audio(current_audio)
        
        # ì €ì¥
        sf.write(output_path, current_audio.T, self.sample_rate)
        print(f"\nâœ… All {len(track_paths)} tracks mixed successfully!")
        
        return output_path


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 4:
        print("Usage: python advanced_mixer.py <track1> <track2> <output>")
        sys.exit(1)
    
    track1 = sys.argv[1]
    track2 = sys.argv[2]
    output = sys.argv[3]
    
    mixer = AdvancedMixer()
    mixer.mix(track1, track2, output, 
              sync_beats=True,
              match_tempo=True,
              harmonic_mix=True,
              transition_bars=16,
              transition_style='classic',
              auto_detect=True)
